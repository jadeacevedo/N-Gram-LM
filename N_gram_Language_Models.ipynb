{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadeacevedo/N-Gram-LM/blob/main/N_gram_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Language Models research\n",
        "\n",
        "language modeling is defimes as the task of predicting the next word in a sequence given the previous words. In this project , i will focus on the related problem of predicting the next character in a sequence given the previous characters.\n",
        "\n",
        "The  goals of this project is  to:\n",
        "\n",
        "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
        "- Implement basic smoothing, back-off and interpolation.\n",
        "- Have fun using a language model to probabilistically generate texts!\n",
        "- Use a set of language models to perform text classification."
      ],
      "metadata": {
        "id": "HUqyGGgYG4aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "9wKWi96qcCbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Utility Functions\n",
        "################################################################################\n",
        "\n",
        "COUNTRY_CODES = ['af', 'cn', 'de', 'fi', 'fr', 'in', 'ir', 'pk', 'za']\n",
        "\n",
        "def start_pad(n):\n",
        "    ''' Returns a padding string of length n to append to the front of text\n",
        "        as a pre-processing step to building n-grams '''\n",
        "    return '~' * n\n",
        "\n",
        "def create_ngram_model(model_class, path, n=2, k=0):\n",
        "    ''' Creates and returns a new n-gram model trained on the city names\n",
        "        found in the path file '''\n",
        "    model = model_class(n, k)\n",
        "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
        "        model.update(f.read())\n",
        "    return model\n",
        "\n",
        "def create_ngram_model_lines(model_class, path, n=2, k=0):\n",
        "    ''' Creates and returns a new n-gram model trained on the city names\n",
        "        found in the path file '''\n",
        "    model = model_class(n, k)\n",
        "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            model.update(line.strip())\n",
        "    return model"
      ],
      "metadata": {
        "id": "s35HdvJdcI2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Generating N-Grams"
      ],
      "metadata": {
        "id": "Y8VQx2cGTL0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQMcTj1oGvhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93b8c91-104a-4250-9a40-7f7819e3e47c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('~', 'a'), ('a', 'b'), ('b', 'c')]\n",
            "[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\n",
            "[('', 'a'), ('', 'b'), ('', 'c')]\n",
            "[('~~~', 'a'), ('~~a', 'b'), ('~ab', 'c')]\n"
          ]
        }
      ],
      "source": [
        "def ngrams(n, text):\n",
        "    ''' Returns the ngrams of the text as tuples where the first element is\n",
        "        the length-n context and the second is the character '''\n",
        "\n",
        "    padded_text = start_pad(n) + text\n",
        "    ngram_list = []\n",
        "    for i in range(n, len(padded_text)):\n",
        "        context = padded_text[i-n:i]\n",
        "        char = padded_text[i]\n",
        "        ngram_list.append((context, char))\n",
        "    return ngram_list\n",
        "\n",
        "print(ngrams(1, 'abc'))\n",
        "print(ngrams(2, 'abc'))\n",
        "print(ngrams(0, 'abc'))\n",
        "print(ngrams(3, 'abc'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Creating an N-gram Model\n"
      ],
      "metadata": {
        "id": "5y2YxpH-c2xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Basic N-Gram Model\n",
        "################################################################################\n",
        "\n",
        "class NgramModel(object):\n",
        "    ''' A basic n-gram model using add-k smoothing '''\n",
        "\n",
        "    def __init__(self, n, k):\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.vocab = set()\n",
        "        self.context_counts = defaultdict(lambda:0)\n",
        "        self.sequence_counts = defaultdict(lambda:0)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        ''' Returns the set of characters in the vocab '''\n",
        "\n",
        "        return self.vocab\n",
        "\n",
        "    def update(self, text):\n",
        "        ''' Updates the model n-grams based on text '''\n",
        "\n",
        "        for context, char in ngrams(self.n, text):\n",
        "            self.vocab.add(char)\n",
        "            self.context_counts[context] += 1\n",
        "            self.sequence_counts[(context, char)] += 1\n",
        "\n",
        "\n",
        "    def prob(self, context, char):\n",
        "        ''' Returns the probability of char appearing after context (without smoothing) '''\n",
        "\n",
        "        if context not in self.context_counts or self.context_counts[context] == 0:\n",
        "            return 1.0 / len(self.vocab) if len(self.vocab) > 0 else 0.0\n",
        "        else:\n",
        "            return self.sequence_counts[(context, char)] / self.context_counts[context]"
      ],
      "metadata": {
        "id": "gjrpaXxwcO-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing code here ###\n",
        "m = NgramModel(1, 0)\n",
        "m.update('abab')\n",
        "\n",
        "print(\"after update |abab| : \", m.get_vocab())\n",
        "m.update('abcd')\n",
        "print(\"after update:|abcd| \", m.get_vocab())\n",
        "print(\"P('a'|'b'):\", m.prob('b', 'a'))"
      ],
      "metadata": {
        "id": "vxsbabe76elY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4cfc5da-a856-4010-b8f6-6f746eb5651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after update |abab| :  {'a', 'b'}\n",
            "after update:|abcd|  {'a', 'c', 'd', 'b'}\n",
            "P('a'|'b'): 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have provided a method `random_char(self, context)` which returns a random character according to the probability distribution determined by the given context. Specifically, let $V = \\{w_1, w_2, \\ldots, w_n\\}$ be the vocab, sorted according to Python's natural lexicographic ordering, and let $0\\leq r \\leq 1$ be a random number between $0$ and $1$. The method returns a character $w_i$ such that\n",
        "$$\\sum_{j=1}^{i-1}P(w_j|\\text{context}) \\leq r < \\sum_{j=1}^iP(w_j|\\text{context})$$"
      ],
      "metadata": {
        "id": "nIutCMK28NQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_char(self, context):\n",
        "    ''' Returns a random character based on the given context and the\n",
        "        n-grams learned by this model '''\n",
        "    r = random.random()\n",
        "    pre_sum = 0\n",
        "    for i, char in enumerate(sorted(self.vocab)):\n",
        "        post_sum = pre_sum + self.prob(context, char)\n",
        "        if pre_sum <= r < post_sum:\n",
        "            return char\n",
        "        pre_sum = post_sum\n",
        "\n",
        "NgramModel.random_char = random_char"
      ],
      "metadata": {
        "id": "EnSlL8za8Ldc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = NgramModel(0, 0)\n",
        "m.update('abab')\n",
        "m.update('abcd')\n",
        "random.seed(1)\n",
        "[m.random_char('') for i in range(25)]"
      ],
      "metadata": {
        "id": "J9JKXRLY_IOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8b667e-d3d4-4046-bb28-9e93c10d089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'c',\n",
              " 'c',\n",
              " 'a',\n",
              " 'b',\n",
              " 'b',\n",
              " 'b',\n",
              " 'c',\n",
              " 'a',\n",
              " 'a',\n",
              " 'c',\n",
              " 'b',\n",
              " 'c',\n",
              " 'a',\n",
              " 'b',\n",
              " 'b',\n",
              " 'a',\n",
              " 'd',\n",
              " 'd',\n",
              " 'a',\n",
              " 'a',\n",
              " 'b',\n",
              " 'd',\n",
              " 'b',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_text(self, length):\n",
        "    ''' Returns text of the specified character length based on the\n",
        "        n-grams learned by this model '''\n",
        "    ### TODO ###\n",
        "    result = []\n",
        "    current_context = start_pad(self.n)\n",
        "\n",
        "    for _ in range(length):\n",
        "      if self.n == 0:\n",
        "        char = self.random_char('')\n",
        "      else :\n",
        "        char = self.random_char(current_context)\n",
        "      result.append(char)\n",
        "\n",
        "      if self.n > 0:\n",
        "        current_context = current_context[1:] + char\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "NgramModel.random_text = random_text"
      ],
      "metadata": {
        "id": "0jHnlRhbq2XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m= NgramModel(1, 0)\n",
        "m.update('assignment')\n",
        "m.update('computational')\n",
        "m.update('linguistics')\n",
        "m.update('is')\n",
        "random.seed(1)\n",
        "m.random_text(25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mVbEii_NHUhi",
        "outputId": "947fcb12-47b7-4954-8e08-afc24d566ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'atigngutalintastaticontig'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing Shakespeare\n",
        "\n",
        " training your language model!  \n",
        " First let's grab some text:"
      ],
      "metadata": {
        "id": "QVT1QsjgBOmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "6muxa2wD_lul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb854e47-4ff9-4849-c201-d1247d112452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-16 03:49:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-10-16 03:49:49 (17.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_ngram_model(NgramModel, 'input.txt', 2)\n",
        "m.random_text(250)"
      ],
      "metadata": {
        "id": "PMrgP27IBjTc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4edbc5d2-0667-483c-a631-f632ffb9e85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Fir, ad le the you, suere our nall blove my he any hattles my ledo\\norsend ifin mince: han-lover th I\\nmadebach for lambe viern Gaught,\\nbut sione guill win!\\nTalt?\\nOur he re and groad, thathress,' th: appy heretten I witang niefter:\\nTo racrague che why,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_ngram_model(NgramModel, 'input.txt', 3)\n",
        "m.random_text(250)"
      ],
      "metadata": {
        "id": "ZL8KqGj_O-0G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b904b817-fb37-439f-b8da-72297275b7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Creter wrone, limble Watchese here hear I am I fors! may ture my threethis parder at men;\\nBut lience their\\nand speak mers oddes, unspirity: if des righs.\\nWhichmannot worse have of good keeping ere.\\nGremove's sequity mine o' there recaused you s\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_ngram_model(NgramModel, 'input.txt', 4)\n",
        "m.random_text(250)"
      ],
      "metadata": {
        "id": "BeoctevSPAcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "590a843f-e62d-4d4d-8d5c-5c7bb67af0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizens:\\nThe justice: farm:\\nShe lord:\\nLord, and armour not repent durst; and compassage break'st\\nThat our protes of mind;\\nNot put\\nTo her to take in thank than thou not at hear is their shrewd.\\n\\nQUEEN ELIZABETH:\\nI given in here's is times and h\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = create_ngram_model(NgramModel, 'input.txt', 7)\n",
        "m.random_text(250)"
      ],
      "metadata": {
        "id": "a6_Ax-p7PB7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "089e573e-a5bf-41b6-8f06-c2894396362c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nAy, worthy man: make trial, marshal's true; I hear this ring.\\nHow say you\\nhad not upon recover all they kiss to chide not; for your knowledge,\\nI never standards, set down--\\nAs best beware; thou shalt to this discharged me a graver step\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "\n",
        "*   I could see a distinct trend pertainng to n, the number of characters in the context, has a significant influence.\n",
        "Using a low n (such as 2), the model was quite bad. It created gibberish, simply random letters strung together without any meaning or organisation\n",
        "But the findings became quite more amazing when I raised n to 4 or even 7. The model began producing real words, correct punctuation, and even phrases resembling those from a Shakespeare play. The text started to match the style of the training data and flow naturally. This demonstrated that the model produced far better predictions with more background information, therefore the created text seemed more realistic and less random.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "E5t_pNIWPNRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Perplexity, Smoothing, and Interpolation (6 pts)\n",
        "\n",
        "### Perplexity\n",
        "How do we know whether our language model is good? we use an intrinsic evaluation method called **perplexity**.\n",
        "\n",
        "In this part i will implement the `perplexity(self, text)` method.  A few things to keep in mind:\n",
        "- Numeric underflow will most likely be a problem, so consider using logs\n",
        "- Perplexity is undefined if the language model assigns any zero probabilities to the test set. In that case your code should return positive infinity - `float('inf')`."
      ],
      "metadata": {
        "id": "-cfwaA0LPuj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(self, text):\n",
        "    ''' Returns the perplexity of text based on the n-grams learned by\n",
        "        this model '''\n",
        "    log_prob_sum = 0\n",
        "    if len(text) == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    for context, char in ngrams(self.n, text):\n",
        "        p = self.prob(context, char)\n",
        "        if p == 0:\n",
        "            return float('inf')\n",
        "        log_prob_sum += math.log(p)\n",
        "\n",
        "    avg_log_prob = log_prob_sum / len(text)\n",
        "    perplexity = math.exp(-avg_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "NgramModel.perplexity = perplexity"
      ],
      "metadata": {
        "id": "2YPobO64PLjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = NgramModel(1, 0)\n",
        "m.update('abab')\n",
        "m.update('abcd')\n",
        "print(m.perplexity('abcd'))\n",
        "print(m.perplexity('abca'))\n",
        "print(m.perplexity('abcda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgQz3VliEHbf",
        "outputId": "facb8b03-eefb-4d27-fc70-2b7d73253145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.189207115002721\n",
            "inf\n",
            "1.515716566510398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discuss the perplexity for text that is similar and different from Shakespeare's plays.  \n",
        "\n"
      ],
      "metadata": {
        "id": "PH9FQ1_IRTfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download and unzip the data\n",
        "!wget https://computational-linguistics-class.org/homework/ngram-lms/test_data.zip\n",
        "!unzip test_data.zip"
      ],
      "metadata": {
        "id": "X5fb_6TgRRrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce646c64-4bf7-4798-cce4-711ae00f73e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-16 03:50:31--  https://computational-linguistics-class.org/homework/ngram-lms/test_data.zip\n",
            "Resolving computational-linguistics-class.org (computational-linguistics-class.org)... 185.199.110.153\n",
            "Connecting to computational-linguistics-class.org (computational-linguistics-class.org)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6998 (6.8K) [application/x-zip-compressed]\n",
            "Saving to: ‘test_data.zip’\n",
            "\n",
            "\rtest_data.zip         0%[                    ]       0  --.-KB/s               \rtest_data.zip       100%[===================>]   6.83K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-16 03:50:31 (72.0 MB/s) - ‘test_data.zip’ saved [6998/6998]\n",
            "\n",
            "Archive:  test_data.zip\n",
            "  inflating: nytimes_article.txt     \n",
            "  inflating: shakespeare_sonnets.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### investigate the perplexity for the two different texts ###\n",
        "shakespeare_model = create_ngram_model(NgramModel, 'input.txt', n=4, k=1)\n",
        "\n",
        "with open('nytimes_article.txt', encoding='utf-8', errors='ignore') as f:\n",
        "    nytimes_text = f.read()\n",
        "\n",
        "with open('shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n",
        "    sonnets_text = f.read()\n",
        "perplexity_nytimes = shakespeare_model.perplexity(nytimes_text)\n",
        "perplexity_sonnets = shakespeare_model.perplexity(sonnets_text)\n",
        "\n",
        "\n",
        "print(f\"Perplexity of New York Times article: {perplexity_nytimes}\")\n",
        "print(f\"Perplexity of Shakespeare Sonnets: {perplexity_sonnets}\")"
      ],
      "metadata": {
        "id": "ep1S_aKZSond",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4712e82-622f-4c36-e23c-9a104c1fd6b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of New York Times article: inf\n",
            "Perplexity of Shakespeare Sonnets: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing\n",
        "Laplace Smoothing is described in section 3.5.1 of the [textbook](https://web.stanford.edu/~jurafsky/slp3/).  It adds 1 to each count, and since there are $V$ characters in the vocabulary and each one was incremented, we adjust the denominator to take this into account as well.\n",
        "\n",
        "$$P_{\\text{Laplace}}(w_i) = \\frac{Count(w_i) + 1}{N + |V|}$$\n",
        "\n",
        "A variant of Laplace smoothing is called Add-k smoothing or Add-epsilon smoothing. This is described in section 3.5.2 of the textbook."
      ],
      "metadata": {
        "id": "5thbS_f2Th16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(self, context, char):\n",
        "    ''' Returns the probability of char appearing after context with add-k smoothing '''\n",
        "    ### TODO ###\n",
        "    context_count = self.context_counts[context]\n",
        "    sequence_count = self.sequence_counts[(context, char)]\n",
        "    vocab_size = len(self.vocab)\n",
        "\n",
        "    if vocab_size == 0:\n",
        "        return 0.0\n",
        "\n",
        "    denominator = context_count + self.k * vocab_size\n",
        "    if denominator == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (sequence_count + self.k) / denominator\n",
        "\n",
        "NgramModel.prob = prob"
      ],
      "metadata": {
        "id": "lscCw5bCh9gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = NgramModel(1, 1)\n",
        "m.update('abab')\n",
        "m.update('abcd')\n",
        "m.prob('a', 'a')\n",
        "m.prob('a', 'b')\n",
        "m.prob('c', 'd')\n",
        "m.prob('d', 'a')\n",
        "\n",
        "print(m.prob('a', 'a'))\n",
        "print(m.prob('a', 'b'))\n",
        "print(m.prob('c', 'd'))\n",
        "print(m.prob('d', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHMUoWDEFLDc",
        "outputId": "41b619d3-8014-4d90-f1fc-795ff1fccadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14285714285714285\n",
            "0.5714285714285714\n",
            "0.4\n",
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation\n",
        "\n",
        "The idea of interpolation is to calculate the higher order n-gram probabilities also combining the probabilities for lower-order n-gram models. Like smoothing, this helps us avoid the problem of zeros if we haven't observed the longer sequence in our training data.\n"
      ],
      "metadata": {
        "id": "_9gvDNX1sbdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# N-Gram Model with Interpolation\n",
        "################################################################################\n",
        "\n",
        "class NgramModelWithInterpolation(NgramModel):\n",
        "    ''' An n-gram model with interpolation '''\n",
        "\n",
        "    def __init__(self, n, k):\n",
        "\n",
        "        super().__init__(n, k)\n",
        "        self.models = {}\n",
        "        self.lambdas = [1.0 / (n + 1)] * (n + 1)\n",
        "\n",
        "    def get_vocab(self):\n",
        "\n",
        "        vocab = set()\n",
        "        for model in self.models.values():\n",
        "            vocab.update(model.get_vocab())\n",
        "        return vocab\n",
        "\n",
        "    def update(self, text):\n",
        "\n",
        "        for i in range(self.n + 1):\n",
        "            if i not in self.models:\n",
        "                self.models[i] = NgramModel(i, self.k)\n",
        "            self.models[i].update(text)\n",
        "        self.vocab = self.get_vocab()\n",
        "\n",
        "    def prob(self, context, char):\n",
        "\n",
        "        interpolated_prob = 0.0\n",
        "\n",
        "        for i in range(self.n + 1):\n",
        "            current_context = context[-i:] if i > 0 else \"\"\n",
        "            interpolated_prob += self.lambdas[i] * self.models[i].prob(current_context, char)\n",
        "        return interpolated_prob\n",
        "\n",
        "    def set_lambdas(self, lambdas):\n",
        "        if len(lambdas) != self.n + 1:\n",
        "            raise ValueError(f\"Number of lambdas must be {self.n + 1}\")\n",
        "        if not math.isclose(sum(lambdas), 1.0):\n",
        "             raise ValueError(\"Lambdas must sum to 1.0\")\n",
        "        self.lambdas = lambdas"
      ],
      "metadata": {
        "id": "ZYfbZ3sNsa3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = NgramModelWithInterpolation(1, 0)\n",
        "m.update('abab')\n",
        "m.prob('a', 'a')\n",
        "\n",
        "m.prob('a', 'b')\n",
        "\n",
        "\n",
        "m = NgramModelWithInterpolation(2, 1)\n",
        "m.update('abab')\n",
        "m.update('abcd')\n",
        "m.prob('~a', 'b')\n",
        "\n",
        "m.prob('ba', 'b')\n",
        "\n",
        "m.prob('~c', 'd')\n",
        "\n",
        "m.prob('bc', 'd')\n",
        "print(m.prob('a', 'a'))\n",
        "print(m.prob('a', 'b'))\n",
        "print(m.prob('~a', 'b'))\n",
        "print(m.prob('ba', 'b'))\n",
        "print(m.prob('~c', 'd'))\n",
        "print(m.prob('bc', 'd'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEsLQd3nFwVZ",
        "outputId": "c5f10583-4d47-4404-8788-4c3e9056a65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.24206349206349204\n",
            "0.3849206349206349\n",
            "0.46825396825396826\n",
            "0.43492063492063493\n",
            "0.2722222222222222\n",
            "0.3222222222222222\n"
          ]
        }
      ]
    }
  ]
}